{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Auto-Encoders.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPyLwwYK6GDzG4JLfvbdz+v"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jGnqx0e0NvQE","colab_type":"code","outputId":"347fe21c-ddfa-4a45-bc28-794be9881881","executionInfo":{"status":"ok","timestamp":1586806179385,"user_tz":240,"elapsed":878,"user":{"displayName":"YanPeng Gao","photoUrl":"","userId":"05785196380012894573"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive' )\n","%cd '/content/gdrive/My Drive/Comp4107 Project'\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/My Drive/Comp4107 Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0rmyGeYa-dgp","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline  \n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"br_uE731Gn0v","colab_type":"code","colab":{}},"source":["from tensorflow.python.keras.optimizers import Adam, RMSprop\n","from tensorflow.python.keras.layers import Input, Dense,Lambda, Embedding, Flatten, Dropout, merge, Activation, BatchNormalization, LeakyReLU\n","from tensorflow.python.keras.models import Model,Sequential\n","from tensorflow.python.keras.regularizers import l2\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import regularizers\n","from tensorflow.python.keras import initializers\n","from tensorflow.python.keras.layers import add, concatenate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Upzr4lg0SLd-","colab_type":"code","colab":{}},"source":["#https://stackoverflow.com/questions/44666098/how-to-implement-sparse-mean-squared-error-loss-in-keras MMSE\n","def rmse(y_true, y_pred):\n","    mask_true = K.cast(K.not_equal(y_true, 0), K.floatx())\n","    masked_squared_error = K.square(mask_true * (y_true - y_pred))\n","    masked_mse = K.sum(masked_squared_error, axis=-1) / K.sum(mask_true, axis=-1)\n","    return K.sqrt(masked_mse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3ccHqiu_Y4I","colab_type":"code","colab":{}},"source":["train_X = np.load(\"ML-1M/train1.npy\")\n","test_X = np.load(\"ML-1M/test1.npy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jbyr7sTDTwhf","colab_type":"code","colab":{}},"source":["def run_experiments(models,title, x_train=train_X, x_test=test_X, epochs=150,start=5):\n","  plt.figure(figsize=(10,8))\n","  iters = np.arange(start,epochs)\n","  for i in range(len(models)):\n","    model = models[i][0]\n","    model.compile(optimizer='rmsprop', loss=rmse)\n","    label = models[i][1]\n","    losses = model.fit(x_train, x_train, \n","                epochs=epochs,\n","                batch_size=256,\n","                shuffle=True,\n","                validation_data=(x_train , x_test))\n","    val_loss= np.array(losses.history[\"val_loss\"])\n","    plt.plot(iters,val_loss[start:],label=label)\n","  \n","  plt.legend()\n","  # plt.ylim(top=1.5)\n","  plt.title(title,fontsize=20)\n","  plt.ylabel(\"Validation Loss\")\n","  plt.xlabel(\"Epochs\")\n","  plt.savefig(title+\".png\")\n","\n","  plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E-VbEd65FN1G","colab_type":"text"},"source":["####  One Layer AutoEncoder\n"]},{"cell_type":"code","metadata":{"id":"ln-LNznaPHde","colab_type":"code","colab":{}},"source":["def basicAE(X,encode_dim):\n","    input_user = Input(shape=(X.shape[1],))\n","    encoded = Dense(encode_dim, activation='elu')(input_user)\n","    decoded = Dense(X.shape[1], activation='elu')(encoded)\n","\n","    return Model(input_user, decoded)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWfD1gJlFZSL","colab_type":"code","colab":{}},"source":["# we are masking some of the input ratings to add noise so hopefully\n","# our autoender can improve generalizabity \n","def denoising_AE(X,encode_dim,dropout):\n","    input_user = Input(shape=(X.shape[1],))\n","    noisy_input = Dropout(rate=dropout)(input_user)\n","    encoded = Dense(encode_dim, activation='elu')(noisy_input)\n","    decoded = Dense(X.shape[1], activation='elu')(encoded)\n","    return Model(input_user, decoded)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"leZrVYYWtLyT","colab_type":"code","colab":{}},"source":["\n","experiments = [(basicAE(train_X,350),\"Latent Space 350\"),(basicAE(train_X,100),\"Latent Space 100\"),(basicAE(train_X,200),\"Latent Space 200\"),\n","                (basicAE(train_X,500),\"Latent Space 500\"),\n","                (basicAE(train_X,650),\"Latent Space 650\")]\n","\n","run_experiments(models=experiments,title=\"Latent Space Dimension of Shallow Autoencoder\",x_train=train_X,x_test=test_X,\n","                epochs=200,start=5) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVm1exbQb8zS","colab_type":"code","colab":{}},"source":["experiments = [(denoising_AE(train_X,350,0.1),\"0.1\"),(denoising_AE(train_X,350,0.2),\"0.2\"),\n","               (denoising_AE(train_X,350,0.35),\"0.35\"), (denoising_AE(train_X,350,0.5),\"0.5\"), (denoising_AE(train_X,350,0),\"No dropout\")]\n","\n","run_experiments(models=experiments,title=\"Dropout Rate of Denoising Autoencoder\",x_train=train_X,x_test=test_X,\n","                epochs=200,start=5)  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9znDxdH26_i","colab_type":"text"},"source":["#### Deeper Autoencoders\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"G5MmvpUjZs1M","colab_type":"code","colab":{}},"source":["def deepAE(X,layers,middle_dim,dropout):\n","    model = Sequential()\n","    #encode\n","    for i in range(len(layers)):\n","      if i ==0:\n","        model.add(Dense(layers[i],input_shape=(X.shape[1],), activation='selu'))\n","      else:\n","        model.add(Dense(layers[i], activation='selu'))\n","    \n","    #latent layer\n","    model.add(Dense(middle_dim, activation='selu'))\n","    model.add(Dropout(dropout))\n","    layers.reverse()\n","    #decode\n","    for i in range(len(layers)):\n","      model.add(Dense(layers[i], activation='selu'))\n","\n","    #output\n","    model.add(Dense(X.shape[1], activation='selu'))\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8NlKkm8S5Xle","colab_type":"code","colab":{}},"source":["experiments = [(deepAE(train_X,[600,400],middle_dim=350,dropout=0.5),\"Layers:600,400,350\"),\n","               (deepAE(train_X,[500],middle_dim=350,dropout=0.5),\"Layers:500,350\"),\n","               (deepAE(train_X,[500,700],middle_dim=350,dropout=0.5),\"Layers:500,700,350\"),\n","               (deepAE(train_X,[300,500],middle_dim=750,dropout=0.5),\"Layers:300,500,750\"),\n","               (deepAE(train_X,[200,400],middle_dim=600,dropout=0.5),\"Layers:200,400,600\"),\n","               (deepAE(train_X,[128,256],middle_dim=256,dropout=0.5),\"Layers:128,256,256\"),\n","               (deepAE(train_X,[128,256,256],middle_dim=256,dropout=0.5),\"Layers:128,256,256,256\")]\n","\n","\n","\n","run_experiments(models=experiments,title=\"Dimension sizes of Deeper Autoencoders\",x_train=train_X,x_test=test_X,\n","                epochs=150,start=5)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MakivlQQyXm2","colab_type":"code","colab":{}},"source":["def deepAE_refeeding(X,layers,middle_dim,dropout):\n","    weights=[]\n","    #encode\n","    for i in range(len(layers)):\n","        weights.append(Dense(layers[i], activation='selu'))\n","    #middle\n","    weights.append(Dense(middle_dim, activation='selu' ))\n","    weights.append(Dropout(dropout))\n","    #decode\n","    layers.reverse()\n","    for i in range(len(layers)):\n","        weights.append(Dense(layers[i], activation='selu' ))\n","    weights.append(Dense(X.shape[1], activation='selu' )) #output\n","    \n","    inputX = Input(shape=(X.shape[1],))\n","    x_feed1 = K.identity(inputX)\n","    for i in range(len(weights)):\n","        x_feed1 = weights[i](x_feed1)\n","    output1 = x_feed2 = x_feed1\n","    for i in range(len(weights)):\n","        x_feed2 = weights[i](x_feed2)\n","    output2 = x_feed2\n","    model = Model(inputs=inputX, outputs=output2) \n","    return model\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKxHvLry1Qma","colab_type":"code","colab":{}},"source":["experiments = [(deepAE_refeeding(train_X,[500],350,0.5),\"Refeed - Layers:500,350\"),\n","               (deepAE_refeeding(train_X,[700,500],350,0.5),\"Refeed - Layers:700,500,350\"),\n","               (deepAE_refeeding(train_X,[500],700,0.5),\"Refeed - Layers:500,700\"),\n","               (deepAE(train_X,[500],350,0.5),\"No refeeding - Layers:500,350\"),] \n","\n","run_experiments(models=experiments,title=\"Dense Refeeding Deep Autoencoder\",x_train=train_X,x_test=test_X,\n","                epochs=200,start=5)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ODyuayuklARY","colab_type":"code","colab":{}},"source":["#batch normalized\n","def deepAE_BN(X,layers,middle_dim,dropout):\n","    model = Sequential()\n","    #encode\n","    for i in range(len(layers)):\n","      if i ==0:\n","        model.add(Dense(layers[i],input_shape=(X.shape[1],), activation='selu' ))\n","      else:\n","        model.add(Dense(layers[i], activation='selu' ))\n","      model.add(BatchNormalization())\n","    #latent layer\n","    model.add(Dense(middle_dim, activation='selu' ))\n","    model.add(Dropout(dropout))\n","    layers.reverse()\n","    #decode\n","    for i in range(len(layers)):\n","      model.add(Dense(layers[i], activation='selu' ))\n","      model.add(BatchNormalization())\n","    #output\n","    model.add(Dense(X.shape[1], activation='selu' ))\n","    return model\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztyK0CD4bMAM","colab_type":"code","colab":{}},"source":["\n","experiments = [(deepAE_BN(train_X,[128,256],256,0.5),\"BN - Layers:128,256,256\"),\n","               (deepAE_BN(train_X,[500],350,0.5),\"BN - Layers:500,350\"),\n","               (deepAE_BN(train_X,[700,500],350,0.5),\"BN - Layers:700,500,350\"),\n","               (deepAE_BN(train_X,[500],700,0.5),\"BN - Layers:500,700\"),\n","               (deepAE(train_X,[500],350,0.5),\"No BN - Layers:500,350\"),]\n","\n","run_experiments(models=experiments,title=\"Batch Normalized Deep Autoencoder\",x_train=train_X,x_test=test_X,\n","                epochs=200,start=5)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jqO6hXidSyK-","colab_type":"code","colab":{}},"source":["import time\n","def run_results(ae,args):\n","    final_loss=[]\n","    times =[]\n","    for i in range(1,6):\n","        model = ae(*args)\n","        model.summary()\n","        train_X_fold = np.load(\"ML-1M/train\"+str(i)+\".npy\")\n","        test_X_fold = np.load(\"ML-1M/test\"+str(i)+\".npy\")\n","        model.compile( optimizer='rmsprop', loss=rmse)\n","        t1 = time.process_time()\n","        losses = model.fit(train_X_fold, train_X_fold,\n","                        epochs=300, \n","                        batch_size=256,\n","                        shuffle=True,\n","                        validation_data=(train_X_fold , test_X_fold)) \n","        t2 = time.process_time()\n","        final_loss.append(losses.history[\"val_loss\"][299])\n","        del model #memory saving\n","        times.append(t2-t1)\n","    return np.array(final_loss),np.array(times)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8y-_oy7OSnHM","colab_type":"code","colab":{}},"source":["args=[train_X,500]\n","loos,tims = run_results( basicAE,args)\n","np.save(\"basic_ae_loss.npy\",loos)\n","np.save(\"basic_ae_times.npy\",tims)   \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MKja598MUsg-","colab_type":"code","outputId":"a6d5c3d0-3335-4e3b-c75c-3dab5d809374","executionInfo":{"status":"ok","timestamp":1586802482113,"user_tz":240,"elapsed":1082,"user":{"displayName":"YanPeng Gao","photoUrl":"","userId":"05785196380012894573"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["print(loos.mean(), 2*loos.std()/5**0.5)\n","print(tims.mean(), 2*tims.std()/5**0.5)\n","loos"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.18296211063861847 0.003033946559135974\n","1557.201540491201 11.421088213593924\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0.18089859, 0.18527307, 0.18153077, 0.17880106, 0.18830706])"]},"metadata":{"tags":[]},"execution_count":35}]}]}